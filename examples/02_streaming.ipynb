{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eea282",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 02: Streaming Responses\n",
    "\n",
    "This example demonstrates how to stream LLM responses token by token.\n",
    "Streaming is useful for:\n",
    "- Providing real-time feedback to users\n",
    "- Building interactive chat applications\n",
    "- Reducing perceived latency\n",
    "\n",
    "Shows:\n",
    "- Basic streaming with .stream()\n",
    "- Handling streaming responses\n",
    "- Streaming with different providers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9075c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from langchain_llm import get_api_key, get_logger, get_model_name, load_env_config, setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f40a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging and configuration\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "load_env_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ddef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_openai_example():\n",
    "    \"\"\"Stream responses from OpenAI.\"\"\"\n",
    "    logger.info(\"Running OpenAI streaming example\")\n",
    "\n",
    "    api_key = get_api_key(\"openai\")\n",
    "    model_name = get_model_name(\"openai\") or \"gpt-5-nano\"\n",
    "    # NOTE: GPT-5 models require temperature=1 (except gpt-5-chat-latest which supports other values)\n",
    "    llm = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        api_key=api_key,\n",
    "        temperature=1.0,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"OpenAI Streaming (gpt-5-nano):\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print()\n",
    "\n",
    "    # Stream the response - each chunk arrives as it's generated\n",
    "    for chunk in llm.stream(\"Explain the history of artificial intelligence in 3 paragraphs\"):\n",
    "        # chunk.content contains the new text\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1900477d",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Experiment with different models: `gpt-4o`, `gpt-5-mini`, or `gpt-5`\n",
    "2. Add a character or word counter to track the streaming progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a52903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e57ac74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_anthropic_example():\n",
    "    \"\"\"Stream responses from Anthropic Claude.\"\"\"\n",
    "    logger.info(\"Running Anthropic streaming example\")\n",
    "\n",
    "    api_key = get_api_key(\"anthropic\")\n",
    "    model_name = get_model_name(\"anthropic\") or \"claude-3-haiku-20240307\"\n",
    "    llm = ChatAnthropic(\n",
    "        model=model_name,\n",
    "        api_key=api_key,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Anthropic Streaming (Claude 3 Haiku):\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print()\n",
    "\n",
    "    # Stream the response\n",
    "    for chunk in llm.stream(\"Explain the history of artificial intelligence in 3 paragraphs\"):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a0a96a",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Try changing the `temperature` parameter (0.0 for deterministic, 1.0 for creative)\n",
    "2. Try different Claude models: `claude-3-5-sonnet-20241022`, `claude-haiku-4-5`\n",
    "3. Add a character or word counter to track the streaming progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb12dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907cee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_gemini_example():\n",
    "    \"\"\"Stream responses from Google Gemini.\"\"\"\n",
    "    logger.info(\"Running Gemini streaming example\")\n",
    "\n",
    "    api_key = get_api_key(\"gemini\")\n",
    "    model_name = get_model_name(\"gemini\") or \"gemini-2.0-flash-lite\"\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Google Gemini Streaming (2.0 Flash-Lite):\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print()\n",
    "\n",
    "    # Stream the response\n",
    "    for chunk in llm.stream(\"Explain the history of artificial intelligence in 3 paragraphs\"):\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Try changing the `temperature` parameter (0.0 for deterministic, 1.0 for creative)\n",
    "2. Test other Gemini models: `gemini-2.5-flash`, `gemini-1.5-pro`\n",
    "3. Add a character or word counter to track the streaming progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b41377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_streaming_example():\n",
    "    \"\"\"\n",
    "    More advanced streaming example showing how to:\n",
    "    - Collect chunks for later processing\n",
    "    - Track streaming progress\n",
    "    \"\"\"\n",
    "    logger.info(\"Running advanced streaming example\")\n",
    "\n",
    "    api_key = get_api_key(\"openai\")\n",
    "    # NOTE: GPT-5 models require temperature=1 (except gpt-5-chat-latest which supports other values)\n",
    "    llm = ChatOpenAI(model=get_model_name(\"openai\") or \"gpt-5-nano\", api_key=api_key, temperature=1.0)\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Advanced Streaming Example:\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print()\n",
    "    print(\"Prompt: Write a haiku about AI\")\n",
    "    print(\"\\nResponse:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Collect chunks while streaming\n",
    "    full_response = \"\"\n",
    "    chunk_count = 0\n",
    "\n",
    "    for chunk in llm.stream(\"Write a haiku about AI\"):\n",
    "        content = chunk.content\n",
    "        full_response += content\n",
    "        chunk_count += 1\n",
    "        print(content, end=\"\", flush=True)\n",
    "\n",
    "    print()\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"\\nReceived {chunk_count} chunks\")\n",
    "    print(f\"Total characters: {len(full_response)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Experiment with different models: `gpt-4o`, `gpt-5-mini`, or `gpt-5`\n",
    "2. Add a character or word counter to track the streaming progress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5426a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all streaming examples.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Example 02: Streaming Responses\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nStreaming allows you to see responses as they're generated,\")\n",
    "    print(\"token by token, instead of waiting for the complete response.\")\n",
    "    print(\"\\nWatch the text appear in real-time!\")\n",
    "    print()\n",
    "\n",
    "    # Run examples for each provider\n",
    "    try:\n",
    "        streaming_openai_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"OpenAI streaming example failed: {e}\")\n",
    "        print(f\"[ERROR] OpenAI example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    try:\n",
    "        streaming_anthropic_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Anthropic streaming example failed: {e}\")\n",
    "        print(f\"[ERROR] Anthropic example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    try:\n",
    "        streaming_gemini_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gemini streaming example failed: {e}\")\n",
    "        print(f\"[ERROR] Gemini example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    try:\n",
    "        advanced_streaming_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Advanced streaming example failed: {e}\")\n",
    "        print(f\"[ERROR] Advanced example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"[SUCCESS] Streaming examples complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff998aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
