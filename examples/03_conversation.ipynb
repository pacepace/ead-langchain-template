{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 03: Conversations with Message History\n",
    "\n",
    "This example demonstrates how to maintain conversation history for\n",
    "multi-turn interactions with LLMs.\n",
    "\n",
    "Shows:\n",
    "- Managing message history\n",
    "- Multi-turn conversations\n",
    "- Different message types (system, human, AI)\n",
    "- Conversation with different providers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b29cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d0fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from langchain_llm import get_api_key, get_logger, get_model_name, load_env_config, setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db5043e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging and configuration\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "load_env_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4104c37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_conversation_example():\n",
    "    \"\"\"\n",
    "    Basic conversation with message history.\n",
    "    Shows how to manually manage conversation.\n",
    "    \"\"\"\n",
    "    logger.info(\"Running basic conversation example\")\n",
    "\n",
    "    api_key = get_api_key(\"openai\")\n",
    "    model_name = get_model_name(\"openai\") or \"gpt-5-nano\"\n",
    "    # NOTE: GPT-5 models require temperature=1 (except gpt-5-chat-latest which supports other values)\n",
    "    llm = ChatOpenAI(model=model_name, api_key=api_key, temperature=1.0)\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Basic Conversation Example (OpenAI)\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    # Initialize conversation history\n",
    "    # SystemMessage sets the behavior/context\n",
    "    # HumanMessage represents user input\n",
    "    # AIMessage represents assistant responses\n",
    "    conversation_history = [\n",
    "        SystemMessage(content=\"You are a helpful AI assistant that specializes in explaining technical concepts simply.\"),\n",
    "    ]\n",
    "\n",
    "    # First turn\n",
    "    print(\"Human: What is machine learning?\")\n",
    "    conversation_history.append(HumanMessage(content=\"What is machine learning?\"))\n",
    "\n",
    "    response = llm.invoke(conversation_history)\n",
    "    conversation_history.append(AIMessage(content=response.content))\n",
    "\n",
    "    print(f\"AI: {response.content}\\n\")\n",
    "\n",
    "    # Second turn - AI remembers context\n",
    "    print(\"Human: Can you give me a simple example?\")\n",
    "    conversation_history.append(HumanMessage(content=\"Can you give me a simple example?\"))\n",
    "\n",
    "    response = llm.invoke(conversation_history)\n",
    "    conversation_history.append(AIMessage(content=response.content))\n",
    "\n",
    "    print(f\"AI: {response.content}\\n\")\n",
    "\n",
    "    # Third turn - testing memory\n",
    "    print(\"Human: What was my first question?\")\n",
    "    conversation_history.append(HumanMessage(content=\"What was my first question?\"))\n",
    "\n",
    "    response = llm.invoke(conversation_history)\n",
    "    conversation_history.append(AIMessage(content=response.content))\n",
    "\n",
    "    print(f\"AI: {response.content}\\n\")\n",
    "\n",
    "    print(f\"Total messages in history: {len(conversation_history)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Experiment with different models: `gpt-4o`, `gpt-5-mini`, or `gpt-5`\n",
    "2. Add more conversation turns and see how context affects responses\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your experiments here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_conversation_example():\n",
    "    \"\"\"\n",
    "    Interactive conversation helper function.\n",
    "    You can use this pattern to build chat applications.\n",
    "    \"\"\"\n",
    "    logger.info(\"Running interactive conversation example\")\n",
    "\n",
    "    api_key = get_api_key(\"anthropic\")\n",
    "    model_name = get_model_name(\"anthropic\") or \"claude-3-haiku-20240307\"\n",
    "    llm = ChatAnthropic(model=model_name, api_key=api_key, temperature=0.7)\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Interactive Conversation Helper (Anthropic)\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    # Define a reusable conversation function\n",
    "    def have_conversation(llm_instance, system_prompt: str, turns: list[str]):\n",
    "        \"\"\"\n",
    "        Helper function to have multi-turn conversation.\n",
    "\n",
    "        :param llm_instance: LangChain LLM instance\n",
    "        :ptype llm_instance: ChatOpenAI | ChatAnthropic | ChatGoogleGenerativeAI\n",
    "        :param system_prompt: Initial system prompt\n",
    "        :ptype system_prompt: str\n",
    "        :param turns: list of user messages\n",
    "        :ptype turns: list[str]\n",
    "        :return: List of (human_msg, ai_response) tuples\n",
    "        :rtype: list\n",
    "        \"\"\"\n",
    "        history = [SystemMessage(content=system_prompt)]\n",
    "        conversation = []\n",
    "\n",
    "        for user_message in turns:\n",
    "            history.append(HumanMessage(content=user_message))\n",
    "            response = llm_instance.invoke(history)\n",
    "            history.append(AIMessage(content=response.content))\n",
    "            conversation.append((user_message, response.content))\n",
    "\n",
    "        return conversation\n",
    "\n",
    "    # Use the helper\n",
    "    conversation = have_conversation(\n",
    "        llm,\n",
    "        system_prompt=\"You are a creative writing assistant.\",\n",
    "        turns=[\n",
    "            \"Help me start a story about a robot.\",\n",
    "            \"What should happen next?\",\n",
    "            \"Give me a twist ending.\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Display the conversation\n",
    "    for i, (human, ai) in enumerate(conversation, 1):\n",
    "        print(f\"Turn {i}:\")\n",
    "        print(f\"  Human: {human}\")\n",
    "        print(f\"  AI: {ai[:200]}{'...' if len(ai) > 200 else ''}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Try changing the `temperature` parameter (0.0 for deterministic, 1.0 for creative)\n",
    "2. Try different Claude models: `claude-3-5-sonnet-20241022`, `claude-haiku-4-5`\n",
    "3. Change the prompt and observe how the response varies\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your experiments here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df2458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_provider_conversation():\n",
    "    \"\"\"\n",
    "    Example showing how conversation works same across providers.\n",
    "    \"\"\"\n",
    "    logger.info(\"Running multi-provider conversation example\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Multi-Provider Conversation Comparison\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    # Set up all three providers\n",
    "    providers = []\n",
    "\n",
    "    try:\n",
    "        openai_key = get_api_key(\"openai\")\n",
    "        openai_model = get_model_name(\"openai\") or \"gpt-5-nano\"\n",
    "        # NOTE: GPT-5 models require temperature=1 (except gpt-5-chat-latest which supports other values)\n",
    "        providers.append((\"OpenAI\", ChatOpenAI(model=openai_model, api_key=openai_key, temperature=1.0)))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] OpenAI not available: {e}\")\n",
    "\n",
    "    try:\n",
    "        anthropic_key = get_api_key(\"anthropic\")\n",
    "        anthropic_model = get_model_name(\"anthropic\") or \"claude-3-haiku-20240307\"\n",
    "        providers.append((\"Anthropic\", ChatAnthropic(model=anthropic_model, api_key=anthropic_key, temperature=0.7)))\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Anthropic not available: {e}\")\n",
    "\n",
    "    try:\n",
    "        gemini_key = get_api_key(\"gemini\")\n",
    "        gemini_model = get_model_name(\"gemini\") or \"gemini-2.0-flash-lite\"\n",
    "        providers.append(\n",
    "            (\"Gemini\", ChatGoogleGenerativeAI(model=gemini_model, google_api_key=gemini_key, temperature=0.7))\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"[WARNING] Gemini not available: {e}\")\n",
    "\n",
    "    if not providers:\n",
    "        print(\"[WARNING] No providers available. Please configure at least one API key.\\n\")\n",
    "        return\n",
    "\n",
    "    # Same conversation with all providers\n",
    "    conversation_history = [\n",
    "        SystemMessage(content=\"You are a math tutor.\"),\n",
    "        HumanMessage(content=\"What is 15 * 24?\"),\n",
    "    ]\n",
    "\n",
    "    for provider_name, llm in providers:\n",
    "        print(f\"\\n{provider_name} Response:\")\n",
    "        print(\"-\" * 60)\n",
    "        response = llm.invoke(conversation_history)\n",
    "        print(response.content[:200] + (\"...\" if len(response.content) > 200 else \"\"))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Add more conversation turns and see how context affects responses\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Your experiments here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c24319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all conversation examples.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Example 03: Conversations with Message History\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nThis example shows how to maintain conversation history\")\n",
    "    print(\"for multi-turn interactions with LLMs.\")\n",
    "    print()\n",
    "\n",
    "    try:\n",
    "        basic_conversation_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Basic conversation example failed: {e}\")\n",
    "        print(f\"[ERROR] Basic conversation example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    try:\n",
    "        interactive_conversation_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Interactive conversation example failed: {e}\")\n",
    "        print(f\"[ERROR] Interactive conversation example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    try:\n",
    "        multi_provider_conversation()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Multi-provider conversation example failed: {e}\")\n",
    "        print(f\"[ERROR] Multi-provider conversation example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"[SUCCESS] Conversation examples complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8066b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}