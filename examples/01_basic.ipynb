{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example 01: Basic LLM Usage\n",
    "\n",
    "This example demonstrates the simplest possible way to use LangChain with\n",
    "different providers. Just prompt â†’ response, no complexity.\n",
    "\n",
    "Shows:\n",
    "- Loading configuration\n",
    "- Basic invocation with OpenAI, Anthropic, and Gemini\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c553c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third-party imports\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cfc190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from langchain_llm import get_api_key, get_logger, get_model_name, load_env_config, setup_logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging and configuration\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)\n",
    "load_env_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14963d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_openai_example():\n",
    "    \"\"\"Simple example using OpenAI.\"\"\"\n",
    "    logger.info(\"Running OpenAI basic example\")\n",
    "\n",
    "    # Get API key and model name using our custom config\n",
    "    api_key = get_api_key(\"openai\")\n",
    "    # Use configured model or fall back to cost-effective default\n",
    "    # Configure via EADLANGCHAIN_AI_OPENAI_MODEL in .env\n",
    "    model_name = get_model_name(\"openai\") or \"gpt-5-nano\"\n",
    "\n",
    "    # Create LangChain ChatOpenAI instance\n",
    "    # Default: gpt-5-nano (most cost-effective: $0.05/$0.40 per million tokens)\n",
    "    # For better quality, try: gpt-5, gpt-5-mini, gpt-4o\n",
    "    # NOTE: GPT-5 models require temperature=1 (except gpt-5-chat-latest which supports other values)\n",
    "    llm = ChatOpenAI(\n",
    "        model=model_name,\n",
    "        api_key=api_key,\n",
    "        temperature=1.0,  # Required for gpt-5-nano/gpt-5-mini/gpt-5\n",
    "        # For gpt-5-chat-latest or gpt-4o models, you can use custom temperature:\n",
    "        # temperature=0.7,  # Use this with gpt-5-chat-latest or gpt-4o\n",
    "    )\n",
    "\n",
    "    # Simple invoke - just send a message and get a response\n",
    "    response = llm.invoke(\"What is machine learning in one sentence?\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"OpenAI (gpt-5-nano):\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(response.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863bdf19",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Experiment with different models: `gpt-4o`, `gpt-5-mini`, or `gpt-5`\n",
    "2. Modify the prompt to ask about a topic relevant to your work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_anthropic_example():\n",
    "    \"\"\"Simple example using Anthropic Claude.\"\"\"\n",
    "    logger.info(\"Running Anthropic basic example\")\n",
    "\n",
    "    # Get API key and model name using our custom config\n",
    "    api_key = get_api_key(\"anthropic\")\n",
    "    # Use configured model or fall back to cost-effective default\n",
    "    model_name = get_model_name(\"anthropic\") or \"claude-3-haiku-20240307\"\n",
    "\n",
    "    # Create LangChain ChatAnthropic instance\n",
    "    # Default: claude-3-haiku-20240307 (most cost-effective Claude 3 model)\n",
    "    # For better quality, try: claude-3-5-sonnet-20241022, claude-haiku-4-5\n",
    "    llm = ChatAnthropic(\n",
    "        model=model_name,\n",
    "        api_key=api_key,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Simple invoke - just send a message and get a response\n",
    "    response = llm.invoke(\"What is machine learning in one sentence?\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Anthropic (Claude 3 Haiku):\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(response.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Try changing the `temperature` parameter (0.0 for deterministic, 1.0 for creative)\n",
    "2. Try different Claude models: `claude-3-5-sonnet-20241022`, `claude-haiku-4-5`\n",
    "3. Modify the prompt to ask about a topic relevant to your work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953fc00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_gemini_example():\n",
    "    \"\"\"Simple example using Google Gemini.\"\"\"\n",
    "    logger.info(\"Running Gemini basic example\")\n",
    "\n",
    "    # Get API key and model name using our custom config\n",
    "    api_key = get_api_key(\"gemini\")\n",
    "    # Use configured model or fall back to cost-effective default\n",
    "    model_name = get_model_name(\"gemini\") or \"gemini-2.0-flash-lite\"\n",
    "\n",
    "    # Create LangChain ChatGoogleGenerativeAI instance\n",
    "    # Default: gemini-2.0-flash-lite (most cost-effective: $0.07/$0.30 per million tokens)\n",
    "    # For better quality, try: gemini-2.5-flash, gemini-2.0-flash, gemini-1.5-pro\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Simple invoke - just send a message and get a response\n",
    "    response = llm.invoke(\"What is machine learning in one sentence?\")\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"Google Gemini (2.0 Flash-Lite):\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(response.content)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try This\n",
    "\n",
    "Experiment with the code above:\n",
    "1. Try changing the `temperature` parameter (0.0 for deterministic, 1.0 for creative)\n",
    "2. Test other Gemini models: `gemini-2.5-flash`, `gemini-1.5-pro`\n",
    "3. Modify the prompt to ask about a topic relevant to your work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09e36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Run all basic examples.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Example 01: Basic LLM Usage\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nThis example shows the simplest way to use LangChain with different providers.\")\n",
    "    print(\"Notice how similar the code is for each provider - that's the power of LangChain!\")\n",
    "    print()\n",
    "\n",
    "    # Run examples for each provider\n",
    "    # Comment out providers you don't have API keys for\n",
    "    try:\n",
    "        basic_openai_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"OpenAI example failed: {e}\")\n",
    "        print(f\"[ERROR] OpenAI example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    try:\n",
    "        basic_anthropic_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Anthropic example failed: {e}\")\n",
    "        print(f\"[ERROR] Anthropic example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    try:\n",
    "        basic_gemini_example()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Gemini example failed: {e}\")\n",
    "        print(f\"[ERROR] Gemini example failed ({type(e).__name__}): {e}\\n\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"[SUCCESS] Basic examples complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
